{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 04:51:04,063 - ERROR - Configuration file not found: config.yaml\n",
      "2025-05-07 04:51:04,063 - ERROR - An error occurred during the process. Please check the logs for details.\n",
      "2025-05-07 04:51:04,065 - INFO - Total execution time: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Advanced Model Evaluation with Feature Selection for House Prices\n",
    "\n",
    "# Step 1: Load a house prices dataset from CSV (Assume you have a house_prices.csv ).\n",
    "# Step 2: Apply feature selection and create a train-test split.\n",
    "# Step 3: Train a Lasso Regression model.\n",
    "# Step 4: Perform model evaluation and hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import yaml  # For configuration\n",
    "from typing import Tuple\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)  # Get a specific logger\n",
    "\n",
    "# Custom Exception for Data Validation\n",
    "class DataValidationError(ValueError):\n",
    "    \"\"\"Custom exception to raise when data validation fails.\"\"\"\n",
    "    pass\n",
    "\n",
    "def load_config(config_file='config.yaml'):\n",
    "    \"\"\"Loads configuration parameters from a YAML file.\n",
    "\n",
    "    Args:\n",
    "        config_file (str): Path to the YAML configuration file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the configuration parameters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(config_file, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        logger.info(f\"Configuration loaded from {config_file}\")\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Configuration file not found: {config_file}\")\n",
    "        raise\n",
    "    except yaml.YAMLError as e:\n",
    "        logger.error(f\"Error parsing YAML file: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads the house prices dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading dataset from: {file_path}\")\n",
    "        data = pd.read_csv(file_path)\n",
    "        logger.info(\"Dataset loaded successfully.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Error: The file '{file_path}' was not found. Please check the file path.\")\n",
    "        raise\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.error(f\"Error: The file '{file_path}' is empty.\")\n",
    "        raise\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Error: Failed to parse the file '{file_path}'. Ensure it's a valid CSV format.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "def explore_data(data: pd.DataFrame, target_column: str) -> None:\n",
    "    \"\"\"Explores the dataset and performs data validation.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "        target_column (str): The name of the target column.\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n--- Dataset Exploration and Validation ---\")\n",
    "    logger.info(\"Dataset Information:\")\n",
    "    data.info()\n",
    "    logger.info(\"\\nFirst 5 rows of the dataset:\")\n",
    "    logger.info(data.head())\n",
    "    logger.info(\"\\nBasic statistics of the dataset:\")\n",
    "    logger.info(data.describe())\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = data.isnull().sum()\n",
    "    logger.info(\"\\nMissing values per column:\\n%s\", missing_values) # Consistent logging\n",
    "    if missing_values.any():\n",
    "        logger.warning(\"Missing values found in the dataset.\")\n",
    "\n",
    "    # Check for duplicate rows\n",
    "    num_duplicates = data.duplicated().sum()\n",
    "    logger.info(f\"\\nNumber of duplicate rows: {num_duplicates}\")\n",
    "    if num_duplicates > 0:\n",
    "        logger.warning(\"Duplicate rows found in the dataset.\")\n",
    "\n",
    "    # Check if the target column exists\n",
    "    if target_column not in data.columns:\n",
    "        raise DataValidationError(f\"Target column '{target_column}' not found in the dataset.\")\n",
    "\n",
    "    # Example data validation: Check for non-numeric values in the target column\n",
    "    if not pd.api.types.is_numeric_dtype(data[target_column]):\n",
    "        raise DataValidationError(f\"Target column '{target_column}' should be numeric.\")\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame, target_column: str, test_size: float, random_state: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Prepares the data for modeling by separating features and target,\n",
    "    handling categorical variables, and splitting into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "        target_column (str): The name of the target column.\n",
    "        test_size (float): Proportion of the data to use for testing.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, StandardScaler]:\n",
    "            Tuple containing training and testing features, training and testing target,\n",
    "            and the StandardScaler used for scaling.\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n--- Data Preprocessing ---\")\n",
    "\n",
    "    # Separate features and target\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Handle categorical features using one-hot encoding\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    logger.info(\"Categorical features encoded using one-hot encoding.\")\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    logger.info(f\"Training set size: {len(X_train)}, Testing set size: {len(X_test)}\")\n",
    "\n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    logger.info(\"Features scaled using StandardScaler.\")\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "def feature_selection(X_train: np.ndarray, y_train: pd.Series, alpha: float, random_state: int, early_stopping=False, cv=5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Selects features using Lasso Regression with optional early stopping.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        alpha (float): Regularization parameter for Lasso.\n",
    "        random_state (int): Random seed.\n",
    "        early_stopping (bool, optional): Whether to use early stopping. Defaults to False.\n",
    "        cv (int): Number of cross-validation folds for early stopping.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray:  Selected training features.\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n--- Feature Selection ---\")\n",
    "    lasso_selector = Lasso(alpha=alpha, random_state=random_state, warm_start=True) # Enable warm starting for early stopping\n",
    "\n",
    "    if early_stopping:\n",
    "        logger.info(\"Using early stopping for feature selection.\")\n",
    "        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "        best_mse = float('inf')\n",
    "        best_features = None\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "            lasso_selector.fit(X_train_fold, y_train_fold)  # Fit on the fold\n",
    "            y_pred_val = lasso_selector.predict(X_val_fold)\n",
    "            mse = mean_squared_error(y_val_fold, y_pred_val)\n",
    "\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_features = lasso_selector.coef_ != 0\n",
    "                logger.info(f\"Improved MSE: {best_mse:.4f}\")\n",
    "            else:\n",
    "                logger.info(f\"MSE did not improve: {mse:.4f}\")\n",
    "                break # Stop if not improving\n",
    "\n",
    "        if best_features is not None:\n",
    "            selected_features_mask = best_features\n",
    "        else:\n",
    "             selected_features_mask = lasso_selector.coef_ != 0\n",
    "    else:\n",
    "        lasso_selector.fit(X_train, y_train)\n",
    "        selected_features_mask = lasso_selector.coef_ != 0\n",
    "\n",
    "    X_train_selected = X_train[:, selected_features_mask]\n",
    "    logger.info(f\"Number of features before selection: {X_train.shape[1]}\")\n",
    "    logger.info(f\"Number of features after Lasso-based selection: {X_train_selected.shape[1]}\")\n",
    "    return X_train_selected, selected_features_mask\n",
    "\n",
    "def train_model(X_train: np.ndarray, y_train: pd.Series, param_grid: dict, cv: int) -> GridSearchCV:\n",
    "    \"\"\"Trains a Lasso Regression model with hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        param_grid (dict): Dictionary of hyperparameters to tune.\n",
    "        cv (int): Number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "        GridSearchCV: Trained GridSearchCV model.\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n--- Model Training ---\")\n",
    "    pipeline = Pipeline([\n",
    "        ('lasso', Lasso(random_state=42))\n",
    "    ])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='neg_mean_squared_error', verbose=1, error_score='raise')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    logger.info(\"Model training completed.\")\n",
    "    return grid_search\n",
    "\n",
    "def evaluate_model(grid_search: GridSearchCV, X_test: np.ndarray, y_test: pd.Series) -> Tuple[float, float]:\n",
    "    \"\"\"Evaluates the trained model on the test set.\n",
    "\n",
    "    Args:\n",
    "        grid_search (GridSearchCV): Trained GridSearchCV model.\n",
    "        X_test (np.ndarray): Testing features.\n",
    "        y_test (pd.Series): Testing target.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Mean squared error and R-squared score on the test set.\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n--- Model Evaluation ---\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    logger.info(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    logger.info(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "    logger.info(f\"R-squared Score on Test Set: {r2:.4f}\")\n",
    "    return mse, r2\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the loading, preprocessing, feature selection,\n",
    "    model training, and evaluation of the house prices dataset.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        config = load_config() # Load config\n",
    "        file_path = config['file_path']\n",
    "        test_size = config['test_size']\n",
    "        random_state = config['random_state']\n",
    "        cv = config['cv']\n",
    "        target_column = config['target_column']\n",
    "        lasso_alpha = config['lasso_alpha']\n",
    "        param_grid = {\n",
    "            'lasso__alpha': np.logspace(config['lasso_alpha_start'], config['lasso_alpha_end'], config['lasso_alpha_num'])\n",
    "        }\n",
    "        use_early_stopping = config.get('use_early_stopping', False) #get early stopping\n",
    "\n",
    "        data = load_data(file_path)\n",
    "        explore_data(data, target_column)\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test, scaler = preprocess_data(data, target_column, test_size, random_state)\n",
    "        X_train_selected, selected_features_mask = feature_selection(X_train_scaled, y_train, lasso_alpha, random_state, early_stopping=use_early_stopping, cv=cv)\n",
    "        X_test_selected = X_test_scaled[:, selected_features_mask] #apply feature selection to test\n",
    "\n",
    "        grid_search = train_model(X_train_selected, y_train, param_grid, cv)\n",
    "        mse, r2 = evaluate_model(grid_search, X_test_selected, y_test)\n",
    "\n",
    "        print(\"\\n--- Summary ---\")\n",
    "        print(f\"Final Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "        print(f\"Final R-squared Score on Test Set: {r2:.4f}\")\n",
    "\n",
    "    except DataValidationError as dve:\n",
    "        logger.error(f\"Data Validation Error: {dve}\")\n",
    "    except Exception:\n",
    "        logger.error(\"An error occurred during the process. Please check the logs for details.\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
